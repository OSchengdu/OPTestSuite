[11:11:35] /home/tanjunhan/MXNet/src/storage/storage.cc:202: Using Pooled (Naive) StorageManager for CPU
Traceback (most recent call last):
  File "/home/tanjunhan/MXNet/lamb_update_phase1.py", line 16, in <module>
    lamb_update_phase1_res = run_performance_test(
                             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/benchmark_utils.py", line 251, in run_performance_test
    benchmark_result = _run_operator_performance_test(op, inputs, run_backward, warmup, runs, kwargs_list, profiler)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/benchmark_utils.py", line 188, in _run_operator_performance_test
    _, _ = benchmark_helper_func(op, warmup, **kwargs_list[0])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/profiler_utils.py", line 200, in cpp_profile_it
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/ndarray_utils.py", line 58, in nd_forward_backward_and_profile
    res = op(**kwargs_new)
          ^^^^^^^^^^^^^^^^
  File "<string>", line 113, in lamb_update_phase1
  File "/home/tanjunhan/MXNet/python/mxnet/_ctypes/ndarray.py", line 76, in _imperative_invoke
    check_call(_LIB.MXImperativeInvoke(
  File "/home/tanjunhan/MXNet/python/mxnet/base.py", line 253, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: MXNetError: Cannot find argument 'lr', Possible Arguments:
----------------
beta1 : float, optional, default=0.899999976
    The decay rate for the 1st moment estimates.
beta2 : float, optional, default=0.999000013
    The decay rate for the 2nd moment estimates.
epsilon : float, optional, default=9.99999997e-07
    A small constant for numerical stability.
t : int, required
    Index update count.
bias_correction : boolean, optional, default=1
    Whether to use bias correction.
wd : float, required
    Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.
rescale_grad : float, optional, default=1
    Rescale gradient to grad = rescale_grad*grad.
clip_gradient : float, optional, default=-1
    Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient <= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).
, in operator lamb_update_phase1(name="", epsilon="1e-08", beta1="0.9", wd="0.0001", beta2="0.999", lr="0.01")

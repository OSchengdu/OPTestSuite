[10:10:08] /home/tanjunhan/MXNet/src/storage/storage.cc:202: Using Pooled (Naive) StorageManager for CPU
Traceback (most recent call last):
  File "/home/tanjunhan/MXNet/Softmax.py", line 12, in <module>
    softmax_res = run_performance_test(
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/benchmark_utils.py", line 251, in run_performance_test
    benchmark_result = _run_operator_performance_test(op, inputs, run_backward, warmup, runs, kwargs_list, profiler)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/benchmark_utils.py", line 188, in _run_operator_performance_test
    _, _ = benchmark_helper_func(op, warmup, **kwargs_list[0])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/profiler_utils.py", line 200, in cpp_profile_it
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/ndarray_utils.py", line 58, in nd_forward_backward_and_profile
    res = op(**kwargs_new)
          ^^^^^^^^^^^^^^^^
  File "<string>", line 150, in Softmax
  File "/home/tanjunhan/MXNet/python/mxnet/_ctypes/ndarray.py", line 76, in _imperative_invoke
    check_call(_LIB.MXImperativeInvoke(
  File "/home/tanjunhan/MXNet/python/mxnet/base.py", line 253, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: MXNetError: Cannot find argument 'axis', Possible Arguments:
----------------
grad_scale : float, optional, default=1
    Scales the gradient by a float factor.
ignore_label : float, optional, default=-1
    The instances whose `labels` == `ignore_label` will be ignored during backward, if `use_ignore` is set to ``true``).
multi_output : boolean, optional, default=0
    If set to ``true``, the softmax function will be computed along axis ``1``. This is applied when the shape of input array differs from the shape of label array.
use_ignore : boolean, optional, default=0
    If set to ``true``, the `ignore_label` value will not contribute to the backward gradient.
preserve_shape : boolean, optional, default=0
    If set to ``true``, the softmax function will be computed along the last axis (``-1``).
normalization : {'batch', 'null', 'valid'},optional, default='null'
    Normalizes the gradient.
out_grad : boolean, optional, default=0
    Multiplies gradient with output gradient element-wise.
smooth_alpha : float, optional, default=0
    Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.
, in operator SoftmaxOutput(name="", axis="1")

[11:34:53] /home/tanjunhan/MXNet/src/storage/storage.cc:202: Using Pooled (Naive) StorageManager for CPU
Traceback (most recent call last):
  File "/home/tanjunhan/MXNet/mp_sgd_mom_update.py", line 17, in <module>
    mp_sgd_mom_update_res = run_performance_test(
                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/benchmark_utils.py", line 251, in run_performance_test
    benchmark_result = _run_operator_performance_test(op, inputs, run_backward, warmup, runs, kwargs_list, profiler)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/benchmark_utils.py", line 188, in _run_operator_performance_test
    _, _ = benchmark_helper_func(op, warmup, **kwargs_list[0])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/profiler_utils.py", line 200, in cpp_profile_it
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tanjunhan/MXNet/benchmark/opperf/utils/ndarray_utils.py", line 58, in nd_forward_backward_and_profile
    res = op(**kwargs_new)
          ^^^^^^^^^^^^^^^^
  File "<string>", line 75, in mp_sgd_mom_update
  File "/home/tanjunhan/MXNet/python/mxnet/_ctypes/ndarray.py", line 76, in _imperative_invoke
    check_call(_LIB.MXImperativeInvoke(
  File "/home/tanjunhan/MXNet/python/mxnet/base.py", line 253, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: MXNetError: Cannot find argument 'grad32', Possible Arguments:
----------------
lr : float, required
    Learning rate
momentum : float, optional, default=0
    The decay rate of momentum estimates at each epoch.
wd : float, optional, default=0
    Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.
rescale_grad : float, optional, default=1
    Rescale gradient to grad = rescale_grad*grad.
clip_gradient : float, optional, default=-1
    Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient <= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).
lazy_update : boolean, optional, default=1
    If true, lazy updates are applied if gradient's stype is row_sparse and both weight and momentum have the same stype
, in operator mp_sgd_mom_update(name="", momentum="0.9", lr="0.01", wd="0.0001", grad32="
[[-8.8339078e-01 -1.3761734e+00  1.1248538e+00 ... -3.6660042e-01
  -1.1795151e+00 -2.6304355e+00]
 [-6.3538688e-01  2.5222316e-01  1.6202663e+00 ...  8.0203539e-01
   3.1116366e-01  1.1064993e+00]
 [-1.0677394e-01 -8.1723654e-01 -1.0275574e-01 ...  1.0398166e+00
  -3.1820256e-02  9.5327085e-01]
 ...
 [-9.9135292e-01 -7.1518075e-01 -1.3935946e+00 ...  1.3507289e+00
  -3.3484676e-04  6.4062113e-01]
 [ 8.9867318e-01  2.0776839e+00 -1.4388790e+00 ...  1.7655078e+00
  -5.1456082e-01  1.3428214e+00]
 [-1.1801084e+00  8.0413687e-01  2.4010225e-01 ...  1.5303963e+00
  -6.3632691e-01 -1.2182810e+00]]
<NDArray 1024x1024 @cpu(0)>")
